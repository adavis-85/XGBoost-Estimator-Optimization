# XGBoost Estimator Optimization

A demonstration of how to find the optimal number of estimators for an XGBoost model to achieve the highest or lowest metric of your choosing.  Similar to early-stopping (which is also available) this can be used to avoid training a model beyond what is needed.  Uploaded as a notebook to show full out put for both training and validation.  This was done with mortgage data (binary classification, eligible or not eligible) but the data is up to the user to dive into and process accordingly. The performance of the optimal estimators (trees) of the model is then compared to standard hyperparameter tuning in terms of f-score and auc as well.   
